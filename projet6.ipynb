{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLgvPv31Yd6B"
   },
   "source": [
    "# Project 6\n",
    "---\n",
    "\n",
    "In this project, we will tackle a multi-class image classification problem using Convolutional Neural Networks (CNNs). The dataset contains color images that belong to 10 different classes.\n",
    "\n",
    "It is split into:\n",
    "- 50000 images for training\n",
    "- 10000 images for testing\n",
    "\n",
    "We'll also reserve 10000 images from the training set for validation. So the final split will be:\n",
    "- 40000 images for training\n",
    "- 10000 for validation\n",
    "- 10000 for testing\n",
    "\n",
    "**1. First CNN architecture implementation** \n",
    "\n",
    "we will create a CNN architecture and train our model and compare the results (in terms of accuracy and loss) for training and validation dataset. Then, we will identify the overfitting issue in this model.\n",
    "**2. Mitigating overfitting** \n",
    "\n",
    "To reduce overfitting problem, we will implement:\n",
    "* Dropout \n",
    "* Norm penalization \n",
    "* Both Dropout and Norm penalization \n",
    "* Your proposed model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip5pgmLL_1CR"
   },
   "source": [
    "# 0. Importing packages and preparing data\n",
    "\n",
    "Necessary libraries for data handling, modeling, and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8ICvSmnmag9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmv0G8-9JtFh"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16511,
     "status": "ok",
     "timestamp": 1744122872626,
     "user": {
      "displayName": "Secil ERCAN",
      "userId": "15681917378975218022"
     },
     "user_tz": -120
    },
    "id": "sfw_XWeLJr0g",
    "outputId": "2a8aceb9-b522-4fb1-8148-54a86d576f55"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "new_directory = \"/content/drive/My Drive/Colab Notebooks/DeepLearningAtelier-E3FD/TP6 MiniProjet/\"  # Change this path\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2xppxSZABGI"
   },
   "source": [
    "The dataset CIFAR10 contains 10-classes of objects from `keras.datasets`.\n",
    "\n",
    "Each image belongs to one of the following 10 categories:\n",
    "\n",
    "1. Airplane\n",
    "2. Automobile\n",
    "3. Bird\n",
    "4. Cat\n",
    "5. Deer\n",
    "6. Dog\n",
    "7. Frog\n",
    "8. Horse\n",
    "9. Ship\n",
    "10. Truck\n",
    "\n",
    "We have 5000 images for each class in training dataset, whereas we have 1000 images for each class in test dataset (a balanced dataset).\n",
    "\n",
    "Now, let's load CIFAR-10 dataset, which is already divided into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-Evs-O1mag_"
   },
   "outputs": [],
   "source": [
    "(data_train, label_train), (data_test, label_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMlJ8e7mmahA"
   },
   "outputs": [],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqoT_xqn13Dl"
   },
   "source": [
    "We see the dimension of each image is (32, 32, 3), which means each image has a resolution of 32x32 pixels and 3 color channels (RGB).\n",
    "\n",
    "Let's check some image examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4_4-f4EmahB"
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34DcUNMxmahC"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    img = cv2.resize(data_train[i], (32,32), interpolation=cv2.INTER_CUBIC)\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(class_names[label_train[i][0]])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCYOm0utFv2L"
   },
   "source": [
    "Since the images in 32x32 pixels have low resolution, we can improve their quality by increasing the dimensions to 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFqEdgmXmahC"
   },
   "outputs": [],
   "source": [
    "new_size = (64, 64)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    img = cv2.resize(data_train[i], new_size, interpolation=cv2.INTER_CUBIC)\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(class_names[label_train[i][0]])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlRZPafSGGFb"
   },
   "source": [
    "Even though these are still not in perfect resolution, we will resize our images to 64x64 pixels for the rest of this assignment.\n",
    "\n",
    "We have already training and test dataset coming from `keras.datasets.cifar10`. Now, we will also split our training set into training and validation dataset in order to have finally three sets:\n",
    "1. Training dataset\n",
    "2. Validation dataset\n",
    "3. Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpWOz5B3mahD"
   },
   "outputs": [],
   "source": [
    "x_train = np.array([cv2.resize(img, new_size, interpolation=cv2.INTER_CUBIC) for img in data_train[:40000]])\n",
    "x_val = np.array([cv2.resize(img, new_size, interpolation=cv2.INTER_CUBIC) for img in data_train[40000:]])\n",
    "x_test = np.array([cv2.resize(img, new_size, interpolation=cv2.INTER_CUBIC) for img in data_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiXyr389Gkb9"
   },
   "source": [
    "Since we have classes such as 0, 1, 2, ..., 9 for the labels, we need to categorically encode these labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tg6GAb4mahD"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(label_train[:40000], 10)\n",
    "y_val = to_categorical(label_train[40000:], 10)\n",
    "y_test = to_categorical(label_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxOz23u_IAhP"
   },
   "source": [
    "Let's check the dimensions of our sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1743951947688,
     "user": {
      "displayName": "Secil ERCAN",
      "userId": "15681917378975218022"
     },
     "user_tz": -120
    },
    "id": "mN3eZhXOmahD",
    "outputId": "60636859-7cf1-4847-c56f-91455431311a"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQg-Y2M-0Vb9"
   },
   "source": [
    "Pre-processing for images before using them in our model\n",
    "(normalization of the data and preparing generators with batches of pre-processed tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTMHwrBFmahD"
   },
   "outputs": [],
   "source": [
    "train_datagen  = image.ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9fiY18UmahE"
   },
   "outputs": [],
   "source": [
    "batch_size_gen = 64\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size_gen)\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        x_val, y_val,\n",
    "        batch_size=batch_size_gen)\n",
    "\n",
    "test_generator = test_datagen.flow(\n",
    "        x_test, y_test,\n",
    "        batch_size=batch_size_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQSbnl9qofsx"
   },
   "source": [
    "# 1. First CNN architecture implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YFFEt8qndyj"
   },
   "source": [
    "## 1.1 Model construction \n",
    "\n",
    "\n",
    "To Create the code below with the Keras implementation of the following convolutional network:\n",
    "\n",
    "- Convolutional layer with **32 units**, kernel `(3, 3)`, padding=`'same'`, and `'relu'` activation. *Don't forget to set `input_shape`!*\n",
    "\n",
    "- Max pooling with `(2, 2)` downsampling.\n",
    "\n",
    "- Convolutional layer with **64 units**, kernel `(3, 3)`, and `'relu'` activation.\n",
    "\n",
    "- Max pooling with `(2, 2)` downsampling.\n",
    "\n",
    "- Convolutional layer with **128 units**, kernel `(3, 3)`, and `'relu'` activation.\n",
    "\n",
    "- Max pooling with `(2, 2)` downsampling.\n",
    "\n",
    "- Convolutional layer with **256 units**, kernel `(3, 3)`, and `'relu'` activation.\n",
    "\n",
    "- Max pooling with `(2, 2)` downsampling.\n",
    "\n",
    "- Flatten layer.\n",
    "\n",
    "- Dense layer with **128 units** and `'relu'` activation.\n",
    "\n",
    "- Dense layer with **64 units** and `'relu'` activation.\n",
    "\n",
    "- Dense layer for the output layer (You need to decide **the number of unit (neurons)** and **the activation function**)\n",
    "\n",
    "\n",
    "we Compile the model with the proper **loss function for multi-class clasification**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjOGy7_DmahE"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with the proper loss function for multi-class clasification\n",
    "model.compile(optimizer='adam', loss='...', metrics=['acc'])   # ADD CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybppdBCFQygk"
   },
   "source": [
    "## 1.2 Training\n",
    "\n",
    "we Train the model by executing the following code.\n",
    "\n",
    "we Note that if we do not explicitly give the steps per epoch for training, the model calculates it automatically via the number of data / batch size which is 40000/64 = 625 in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nh7EU115mahE"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hLy2RXTRLTj"
   },
   "source": [
    "## 1.3 Evaluation\n",
    "\n",
    "we Evaluate model by executing the following code.\n",
    "\n",
    "Similar to training step, if we do not explicitly give the steps for test, the model will calculate it automatically via the number of test data / batch size, which is 10000/64 = 156.25, rounded up to 157, in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REiHFy76mahE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc0uWNsQRwo-"
   },
   "source": [
    "Let's plot the loss and accuracy of the model over the training and validation data during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fsXnp3TmahE",
    "outputId": "5b7971c7-7c7d-4bfc-f66f-03393158cf0e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pqFYa3DhKs6"
   },
   "source": [
    "## 1.4 Save model\n",
    "we Save your model and history in your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qqvS_9cmahE"
   },
   "outputs": [],
   "source": [
    "model.save(\"model_original.keras\")\n",
    "\n",
    "with open(\"history_original.pickle\", 'wb') as f:\n",
    "   pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To verify the saved model if you use Colab:\n",
    "# !ls \"/content/drive/MyDrive/Colab Notebooks/DeepLearningAtelier-E3FD/TP6 MiniProjet/\" # Change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWM2ls2UmahF"
   },
   "outputs": [],
   "source": [
    "# Clean the memory\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0eB3lh4OILo"
   },
   "source": [
    "Whenever we need to reload our model and/or history, we can use the following code (by changing the name of your .keras an .pickle file). Not only for this model in Section 1.1 but also the models in Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrgBXceROIW4"
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, name):\n",
    "        with open(name, 'rb') as file:\n",
    "            h = pickle.load(file)\n",
    "            self.history = h\n",
    "\n",
    "model = load_model(\"model_original.keras\")     # Change the file name\n",
    "history = History(\"history_original.pickle\")   # Change the file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8v0xoX_mahF"
   },
   "source": [
    "# Mitigating Overfitting\n",
    "\n",
    "When a model learns too well on the training data but performs poorly on new data, it's overfitting. Regularization methods like Dropout and norm penalization help avoid this by making the model more general.\n",
    "\n",
    "Even if the difference is not huge between training and validation data results, there is still a bit of overfitting in our model. Let’s now try to fix that problem using some regularization methods.\n",
    "\n",
    "In each exercise of mitigating overfitting (2.1, 2.2, 2.3, 2.4), after you construct the model (with necessary modifications, e.g. `Dropout` and/or `kernel_regularizer`, etc.), you will repeat the steps for training, evaluation, and saving the model.\n",
    "\n",
    "Briefly,\n",
    "\n",
    "**CONSTRUCTION --> TRAINING --> EVALUATION --> SAVE MODEL**\n",
    "\n",
    "Now,we apply these steps for the followings:\n",
    "\n",
    "**2.1 Dropout**\n",
    "\n",
    "**2.2 Norm penalization**\n",
    "\n",
    "**2.3 Dropout & Norm penalization**\n",
    "\n",
    "**2.4 our proposed model**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znpDCAcnpAu3"
   },
   "source": [
    "## 2.1 Dropout \n",
    "\n",
    "Keeping the same model construction (we will copy-paste the model we constructed in Section 1.1), add\n",
    "\n",
    "\n",
    "\n",
    "*   one `Dropout` layer after the third convolution layer (Conv2D) with a rate of `0.2`\n",
    "*   one `Dropout` layer after the first dense layer with a rate of `0.5`\n",
    "\n",
    "Applying these two `Dropout` together on the previously given CNN model (in Section 1.1), we repeat the training, evaluation and saving steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8AYj-QKmahF"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# BEGIN CODE HERE\n",
    "\n",
    "\n",
    "# END CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with the proper loss function for multi-class clasification\n",
    "model.compile(optimizer='adam', loss='...', metrics=['acc'])   # ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf1wh9DimahF"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRq4eF8NmahF"
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtYcZwMDxKZJ"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7XHn2_ymahF"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"model_dropout.keras\")\n",
    "\n",
    "with open(\"history_dropout.pickle\", 'wb') as f:\n",
    "   pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To verify the saved model if you use Colab:\n",
    "# !ls \"/content/drive/MyDrive/Colab Notebooks/DeepLearningAtelier-E3FD/TP6 MiniProjet/\" # Change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywqMvMcrmahF",
    "outputId": "65dceb64-8571-4d93-a515-fbcc41707e7d"
   },
   "outputs": [],
   "source": [
    "# Clean the memory\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALbhD0J9mahG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Norm penalization \n",
    "\n",
    "Keeping the same model construction (we will copy-paste the model we constructed in Section 1.1), we add\n",
    "\n",
    "*   `kernal_regularizer` argument to the first dense layer with an L2 of `0.01`\n",
    "\n",
    "Applying this regularization on the previously given CNN model (in Section 1.1), we repeat the training, evaluation and saving steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jt8CbIcTmahG"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# BEGIN CODE HERE\n",
    "\n",
    "\n",
    "# END CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with the proper loss function for multi-class clasification\n",
    "model.compile(optimizer='adam', loss='...', metrics=['acc'])   # ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0NfZ5iwmahG"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpwK0G88mahG"
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIu-tl7TxMRZ"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1u9vIqkMmahG"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"model_norm.keras\")\n",
    "\n",
    "with open(\"history_norm.pickle\", 'wb') as f:\n",
    "   pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To verify the saved model if you use Colab:\n",
    "# !ls \"/content/drive/MyDrive/Colab Notebooks/DeepLearningAtelier-E3FD/TP6 MiniProjet/\" # Change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJ50LubHmahG",
    "outputId": "c10a302c-1374-4ce8-f7d3-3e68a17b8ea0"
   },
   "outputs": [],
   "source": [
    "# Clean the memory\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnW4ctoDmahH"
   },
   "source": [
    "## 2.3 Dropout & Norm penalization \n",
    "\n",
    "we Combine the `Dropout` layers and `kernel_regularizer` that we have already applied in the previous two subsections.\n",
    "\n",
    "Keeping the same model construction (you will copy-paste the model you constructed in Section 1.1), we add\n",
    "\n",
    "\n",
    "*   one `Dropout` layer after the third convolution layer (Conv2D) with a rate of `0.2`\n",
    "*   one `Dropout` layer after the first dense layer with a rate of `0.5`\n",
    "*   `kernal_regularizer` argument to the first dense layer with an L2 of `0.01`\n",
    "\n",
    "Applying them together on the previously given CNN model (in Section 1.1), we repeat the training, evaluation and saving steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xN9J-K1NmahH"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# BEGIN CODE HERE\n",
    "\n",
    "\n",
    "# END CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with the proper loss function for multi-class clasification\n",
    "model.compile(optimizer='adam', loss='...', metrics=['acc'])   # ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDx5YsL-mahH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIMfQ9h4mahH"
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M83-T3D6xOJa"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y46_wgsfmahH"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"model_dropout_norm.keras\")\n",
    "\n",
    "with open(\"history_dropout_norm.pickle\", 'wb') as f:\n",
    "   pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To verify the saved model if you use Colab:\n",
    "# !ls \"/content/drive/MyDrive/Colab Notebooks/DeepLearningAtelier-E3FD/TP6 MiniProjet/\" # Change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4pwI0A6mahH",
    "outputId": "8ed3efc6-4f7c-4237-a164-86a44fda549f"
   },
   "outputs": [],
   "source": [
    "# Clean the memory\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F95eeazpIIvg"
   },
   "source": [
    "## 2.4 our other proposed model \n",
    "\n",
    "Now, we will propose an other  model.\n",
    "\n",
    "we can either add any Dropout layer and/or kernel regularization OR we can rebuild your model with different convolutional and dense layers.\n",
    "\n",
    "we are free to play with any hyperparameters. we Just keep in mind that if we have more complex models or use more epochs, the training step will take a lot of time. Conversely, if we have a small model, the model may not have a good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmL75-5bIJGg"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# BEGIN CODE HERE\n",
    "\n",
    "\n",
    "# END CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with the proper loss function for multi-class clasification\n",
    "model.compile(optimizer='adam', loss='...', metrics=['acc'])   # ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P880rjSuvcY1"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxoTOpr_vchb"
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbWPTxSaxP3B"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQ3naYEavcp9"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"model_proposed.keras\")\n",
    "\n",
    "with open(\"history_proposed.pickle\", 'wb') as f:\n",
    "   pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To verify the saved model if you use Colab:\n",
    "# !ls \"/content/drive/MyDrive/Colab Notebooks/DeepLearningAtelier-E3FD/TP6 MiniProjet/\" # Change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgLX155wvdRc"
   },
   "outputs": [],
   "source": [
    "# Clean the memory\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd8r6mSTmahO"
   },
   "source": [
    "## 2.5 Prediction on an image\n",
    "\n",
    "Now, we can use any trained model that we saved to predict the label of an image. This subsection is not a part of evaluation, but for a prediction on any new image to see the performance of our model.\n",
    "\n",
    "For this task, we can download an image among the list ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') or we can take a picture of it by yourselves.\n",
    "\n",
    "we do not need to modify any code here. we will just:\n",
    "\n",
    "*   change the model name while loading it\n",
    "*   put a new image in your directory where this python file places in and replace the code with its name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PMl3CC5mahO"
   },
   "outputs": [],
   "source": [
    "def detect_object(img_path, model):\n",
    "\n",
    "    img = image.load_img(img_path)\n",
    "    img_resized = img.resize((64, 64))\n",
    "\n",
    "    img_array = image.img_to_array(img_resized) / 255.0\n",
    "    img_array = img_array.reshape(1, 64, 64, 3)\n",
    "\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    pred = model.predict(img_array)\n",
    "    predicted_class = np.argmax(pred)\n",
    "\n",
    "    label_image = class_names[predicted_class]\n",
    "\n",
    "    # plt.imshow(img)\n",
    "    plt.imshow(img_resized)\n",
    "    plt.title(f\"Predicted: {label_image} with a probability of {100 * np.max(pred):.2f}%\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rojqce56HxCx"
   },
   "source": [
    "Recall your model that you will use for the prediction. This can be the model at Section 2.3 Dropout & Norm penalization or the one at Section 2.4 that you have constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6SII334paoe"
   },
   "outputs": [],
   "source": [
    "model = models.load_model(\"model_dropout_norm.keras\")  # Change file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g14OY46mahO"
   },
   "outputs": [],
   "source": [
    "detect_object(\"example1.jpg\", model)  # Replace with your image name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
